{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_list = ['MayoClinic_10_8']  #keyword\n",
    "#webMD_10_8,'ClevelandClinic_10_8','MayoClinic_10_8'\n",
    "startID_list = ['917032337422118914'] #twitter ID  https://twitter.com/MikeLevinCA/status/913394885352349704\n",
    "#917041903790690304,'917041647602556930','917032337422118914'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First test with neurodiversity microsoft since:2015-04-30 until:2017-04-30\n",
    "# https://twitter.com/search?l=en&q=neurodiversity%20microsoft%20%20since%3A2015-01-01%20until%3A2017-05-31&src=typd&lang=en\n",
    "def connectTwitter(company,startID):\n",
    "    #url_list = ['https://twitter.com/WebMD', 'https://twitter.com/ClevelandClinic', 'https://twitter.com/MayoClinic']\n",
    "    url = 'https://twitter.com/MayoClinic'\n",
    "    param = {\n",
    "        'vertical':'default',\n",
    "        #'q':'since:2017-09-01 until:2017-10-01 ',# since:2015-01-01 until:2017-05-31 #time\n",
    "        'l':'en',\n",
    "        'src':'typd',\n",
    "        'include_available_features':'1',\n",
    "        'include_entities':'1',\n",
    "        'lang':'en',\n",
    "        'max_position': startID,\n",
    "        'reset_error_state':'false'\n",
    "    } # TWEET-857-857 means starting with the first page.\n",
    "    header = {\n",
    "        'accept':'application/json, text/javascript, */*; q=0.01',\n",
    "        'accept-encoding':'gzip, deflate, sdch, br',\n",
    "        'accept-language':'en,zh-CN;q=0.8,zh;q=0.6,en-US;q=0.4',\n",
    "        'dnt':'1',\n",
    "        'referer':'https://twitter.com/MayoClinic?l=&q=&src=typd&vertical=&max_position=&',# Remove the information that\n",
    "                                                                                           # param has included.\n",
    "        'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "        'x-requested-with':'XMLHttpRequest',\n",
    "        'x-twitter-active-user':'yes'\n",
    "    }\n",
    "    retry = 0;\n",
    "    while retry < 3:\n",
    "        try:\n",
    "            response = requests.get(url,params=param, headers=header)\n",
    "            html = response.content.decode('utf-8')\n",
    "            soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "            tweets_html_id = soup.find_all('div',class_='stream')[0].find_all('li')\n",
    "            tweets_id = [tweet_html_id.get('data-item-id') for tweet_html_id in tweets_html_id]\n",
    "            tweets_id = [x for x in tweets_id if x is not None]\n",
    "            next_endID = tweets_id[len(tweets_id)-1]\n",
    "\n",
    "        except IndexError:\n",
    "            retry = retry + 1\n",
    "            if retry == 3:\n",
    "                soup = None\n",
    "                next_endID = None\n",
    "            else:\n",
    "                time.sleep(60 * retry)\n",
    "                continue\n",
    "        break\n",
    "\n",
    "    return soup,next_endID,tweets_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DOM Parser\n",
    "def parser_DF(soup):\n",
    "    tweets_html = soup.find_all('li',class_='js-stream-item stream-item stream-item ') # note of the space\n",
    "    \n",
    "    # get twitter ID\n",
    "    tweets_id = [tweet_html.find('div').get('data-tweet-id') \n",
    "                           for tweet_html in tweets_html]\n",
    "    # get user name\n",
    "    tweets_username = [tweet_html.find('div').get('data-screen-name') \n",
    "                           for tweet_html in tweets_html]\n",
    "    # get DataTime\n",
    "    tweets_datetime = []\n",
    "    tweets_datetime_html = [tweet_html.find('small',class_='time') for tweet_html in tweets_html]\n",
    "    for tweet_datetime_html in tweets_datetime_html:\n",
    "        if tweet_datetime_html is not None:\n",
    "            tweets_datetime.append(tweet_datetime_html.find('a').get('title'))\n",
    "        else:\n",
    "            tweets_datetime.append(None)\n",
    "            \n",
    "    # get content\n",
    "    tweets_content = [tweet_html.find('p',class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text').text.strip()\n",
    "                      if tweet_html.find('p',class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text') is not None\n",
    "                      else None for tweet_html in tweets_html]\n",
    "    # get replies, retweets, likes\n",
    "    tweets_replies = [tweet_html.find('span',class_='ProfileTweet-action--reply u-hiddenVisually').find('span').get('data-tweet-stat-count') \n",
    "                      if tweet_html.find('span',class_='ProfileTweet-action--reply u-hiddenVisually') is not None\n",
    "                      else None for tweet_html in tweets_html]\n",
    "    tweets_retweet = [tweet_html.find('span',class_='ProfileTweet-action--retweet u-hiddenVisually').find('span').get('data-tweet-stat-count') \n",
    "                      if tweet_html.find('span',class_='ProfileTweet-action--retweet u-hiddenVisually') is not None\n",
    "                      else None for tweet_html in tweets_html]\n",
    "    tweets_likes = [tweet_html.find('span',class_='ProfileTweet-action--favorite u-hiddenVisually').find('span').get('data-tweet-stat-count') \n",
    "                    if tweet_html.find('span',class_='ProfileTweet-action--favorite u-hiddenVisually') is not None\n",
    "                    else None for tweet_html in tweets_html]\n",
    "    # get image link\n",
    "    tweets_image=[]\n",
    "    tweets_image_html = [tweet_html.find('div',class_='AdaptiveMedia-singlePhoto') for tweet_html in tweets_html]\n",
    "    for tweet_image_html in tweets_image_html:\n",
    "        if tweet_image_html is not None:\n",
    "            tweets_image.append(tweet_image_html.find('div').get('data-image-url'))\n",
    "        else:\n",
    "            tweets_image.append(None)\n",
    "            \n",
    "    # get image of article link        \n",
    "    tweets_a_image=[]\n",
    "    tweets_a_image_html = [tweet_html.find('div',class_='js-media-container') for tweet_html in tweets_html]\n",
    "    for tweet_a_image_html in tweets_a_image_html:\n",
    "        if tweet_a_image_html is not None:\n",
    "            tweets_a_image.append(tweet_a_image_html.find('div').get('data-card-url'))\n",
    "        else:\n",
    "            tweets_a_image.append(None) \n",
    "            \n",
    "    # get video link        \n",
    "    tweets_video=[]\n",
    "    tweets_video_html = [tweet_html.find('div',class_='AdaptiveMedia-videoContainer') for tweet_html in tweets_html]\n",
    "    for tweet_video_html in tweets_video_html:\n",
    "        if tweet_video_html is not None:\n",
    "            tweets_video.append(tweet_video_html.find('div').get('class'))\n",
    "        else:\n",
    "            tweets_video.append(None)     \n",
    "    \n",
    "    # get verified account\n",
    "    #tweets_verified=[]\n",
    "    #tweets_html[6].find('span',class_='Icon Icon--verified')\n",
    "    #tweets_verified_html = [tweet_html.find('span',class_='Icon Icon--verified') for tweet_html in tweets_html]\n",
    "    #for tweet_verified_html in tweets_verified_html:\n",
    "        #if tweet_verified_html is None:\n",
    "            #tweets_verified.append(0)\n",
    "        #else:\n",
    "            #tweets_verified.append(1)\n",
    "    \n",
    "    # store data to dataframe        \n",
    "    tweets = [('username',tweets_username),\n",
    "              ('TwitterID',tweets_id),\n",
    "              #('verified',tweets_verified),\n",
    "              ('DateTime',tweets_datetime),\n",
    "              ('content',tweets_content),\n",
    "              ('likes',tweets_likes),\n",
    "              ('retweet',tweets_retweet),\n",
    "              ('replies',tweets_replies),\n",
    "              ('image',tweets_image),\n",
    "              ('articleImage', tweets_a_image),\n",
    "              ('video',tweets_video)]\n",
    "            \n",
    "\n",
    "    tweets_df = pd.DataFrame.from_items(tweets)\n",
    "    \n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatically get lastID and go to next page\n",
    "companies_html = [] # global variable\n",
    "for i in range(len(company_list)): # for loop of company_list\n",
    "    soup_list=[]\n",
    "    id_list=[]\n",
    "\n",
    "    if len(id_list)==0: # First page: startID and endID would be the same\n",
    "        s = connectTwitter(company_list[i],startID_list[i])\n",
    "        soup_list.append(s[0])\n",
    "        id_list.append(s[1])\n",
    "\n",
    "    if len(id_list)>=1:\n",
    "        while(id_list[-1] is not None):\n",
    "            s = connectTwitter(company_list[i],id_list[-1])\n",
    "            soup_list.append(s[0])\n",
    "            id_list.append(s[1])\n",
    "\n",
    "    if(id_list[-1] is None): # because of the try catch in the def\n",
    "        del soup_list[-1]\n",
    "        del id_list[-1]\n",
    "        \n",
    "    companies_html.append(soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "# parse html and merge dataframe ---CSV\n",
    "companies_df_list = [] # global variable\n",
    "for j in range(len(companies_html)):\n",
    "    tweets_df_list = []\n",
    "    i=0\n",
    "    for soup in companies_html[j]:\n",
    "        print(i)\n",
    "        tweets_df = parser_DF(soup)\n",
    "        tweets_df_list.append(tweets_df)\n",
    "        i=i+1\n",
    "    companies_df_list.append(pd.concat(tweets_df_list,ignore_index=True))\n",
    "    companies_df_list[j].to_csv(company_list[j]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>TwitterID</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>content</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweet</th>\n",
       "      <th>replies</th>\n",
       "      <th>image</th>\n",
       "      <th>articleImage</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MayoClinic</td>\n",
       "      <td>843920329995182080</td>\n",
       "      <td>1:21 PM - 20 Mar 2017</td>\n",
       "      <td>Eight years of education comes down to one day...</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MayoProceedings</td>\n",
       "      <td>843893155682574340</td>\n",
       "      <td>11:33 AM - 20 Mar 2017</td>\n",
       "      <td>CME offered: Discovering new treatments for #G...</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>https://pbs.twimg.com/media/C7YcWi-U0AAqCAS.jpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username           TwitterID                DateTime  \\\n",
       "0       MayoClinic  843920329995182080   1:21 PM - 20 Mar 2017   \n",
       "1  MayoProceedings  843893155682574340  11:33 AM - 20 Mar 2017   \n",
       "\n",
       "                                             content likes retweet replies  \\\n",
       "0  Eight years of education comes down to one day...    12       4       0   \n",
       "1  CME offered: Discovering new treatments for #G...    15      13       0   \n",
       "\n",
       "                                             image articleImage video  \n",
       "0                                             None         None  None  \n",
       "1  https://pbs.twimg.com/media/C7YcWi-U0AAqCAS.jpg         None  None  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(companies_df_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
